# -*- coding: utf-8 -*-
"""Lecture4-Unsolved.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RClhxJh30Qs5rt-sS3BznZvOuWeW5CPB
"""

import keras
from keras import layers
import matplotlib.pyplot as plt
from keras.datasets import cifar10

# Download the dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Check out the shape of data:
print(x_train.shape)
print(f"Range of image data: {x_train.min(), x_train.max()}")
# TODO: See if normalizing the data, to 0-1 floats, helps at all.
# keep in mind we always need to perform same operations to test set,
# which we've already split off above
x_train = x_train.astype('float32')
# x_train /= 255
x_test = x_test.astype('float32')
# x_test /= 255
print(f"New range: {x_train.min(), x_train.max()}")

print(y_train.shape)
from keras.utils import to_categorical
# We need to convert the classes to categorical distributions (they are currently ints)
# TODO: Explain why we can't use the default int
print(y_train[0])
if y_train.shape[1] == 1:
  y_train = to_categorical(y_train)
  y_test = to_categorical(y_test)
print(y_train[0])
print(y_train.shape)

n_classes = y_train.shape[1]
print(f"There are {n_classes} classes in this dataset, based on the number of unique labels.")
class_num_to_str = {
    0: 'airplane',
    1: 'automobile',
    2: 'bird',
    3: 'cat',
    4: 'deer',
    5: 'dog',
    6: 'frog',
    7: 'horse',
    8: 'ship',
    9: 'truck'
}

# TODO: Play with the index idx to see different images in the dataset
import numpy as np
idx = 4902
plt.imshow(x_train[idx] / 255) # if you've already normalized, you can remove this 255
print(f'Label: {y_train[idx]}')
# Convert back to int:
class_num = np.argmax(y_train[idx])
print(f'Label: {class_num}')
print(f"According to the labeled dataset, this is a {class_num_to_str[class_num]}.")

# Let's build a simple convolutional neural network to train on CIFAR-10
model = keras.Sequential(
    [
        # TODO: Input a model architecture
    ]
)
# Hints:
# Start with an input layer and end with a Dense layer with the correct number
# of outputs (num classes). Conv2D is the layer name you (probably) want

try:
  model.summary()
except ValueError:
  print("you have not yet successfully implemented the model (see above cell)")

optimizer = keras.optimizers.Adam(learning_rate=3e-4)
model.compile(optimizer=optimizer,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

batch_size = 512
epochs = 20
history = model.fit(x_train, y_train,
                    epochs=epochs,
                    validation_split=0.2)

def show_train_history(train_history):
    fig=plt.gcf()
    fig.set_size_inches(16, 6)
    plt.subplot(121)
    print(train_history.history.keys())

    if "accuracy" in train_history.history.keys():
        plt.plot(train_history.history["accuracy"])

    if "val_accuracy" in train_history.history.keys():
        plt.plot(train_history.history["val_accuracy"])

    plt.title("Train History")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend(["train", "validation"], loc="upper left")
    plt.subplot(122)

    if "loss" in train_history.history.keys():
        plt.plot(train_history.history["loss"])

    if "val_loss" in train_history.history.keys():
        plt.plot(train_history.history["val_loss"])

    plt.title("Train History")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend(["train", "validation"], loc="upper left")
    plt.show()

show_train_history(history)

# Let's see how it does on the test set:
R = 5
C = 5
fig, axes = plt.subplots(R, C, figsize=(12,12))
axes = axes.ravel()

y_pred = model.predict(x_test[:R*C, :,:,:])
y_pred = np.argmax(y_pred, axis=1)
y_test = np.argmax(y_test, axis=1)
for i in np.arange(0, R*C):
    axes[i].imshow(x_test[i])
    axes[i].set_title("True: %s \nPredict: %s" % (class_num_to_str[y_test[i]], class_num_to_str[y_pred[i]]))
    axes[i].axis('off')
    plt.subplots_adjust(wspace=1)

